
@book{mehrabianApproachEnvironmentalPsychology1974,
	address = {Cambridge},
	title = {An approach to environmental psychology},
	isbn = {978-0-262-13090-5},
	publisher = {M.I.T. Press},
	author = {Mehrabian, Albert and Russell, James A.},
	year = {1974},
	keywords = {Emotions, Environment, Environmental psychology, Psychology, Experimental, Social environment},
}

@article{agianpuye3DFacialExpression2013,
	title = {{3D} {Facial} {Expression} {Synthesis}: {A} {Survey}},
	abstract = {Facial expression synthesis is a process of generating new face shapes from a given face and still retain the distinct facial characteristics of the initial face. The generated facial expressions can be used to improve the performance of existing face identification systems, or to enhance human recognition. Earlier work on synthesizing face shapes used 2D face images. Only recently, the work moved to using 3D face shapes given the availability and improvement in 3D scanner technologies. The advantage of 3D faces over 2D image data is that 3D face holds more geometric shape data and is invariant to poses and illumination. This paper aims to give an overview of the methods used for 3D facial expression synthesis. We present an overview of 3D face expression synthesis, its applications and benefits and then we review some of the most resent 3D face expression synthesis approaches.},
	language = {en},
	journal = {th International Conference on Information Technology in Asia},
	author = {Agianpuye, Samuel and Minoi, Jacey-Lynn},
	year = {2013},
	pages = {7},
	file = {Agianpuye 和 Minoi - 2013 - 3D Facial Expression Synthesis A Survey.pdf:C\:\\Users\\27637\\Zotero\\storage\\FN7UXDBS\\Agianpuye 和 Minoi - 2013 - 3D Facial Expression Synthesis A Survey.pdf:application/pdf},
}

@article{egger3DMorphableFace2020,
	title = {{3D} {Morphable} {Face} {Models}—{Past}, {Present}, and {Future}},
	volume = {39},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/3395208},
	doi = {10.1145/3395208},
	language = {en},
	number = {5},
	urldate = {2020-11-07},
	journal = {ACM Transactions on Graphics},
	author = {Egger, Bernhard and Smith, William A. P. and Tewari, Ayush and Wuhrer, Stefanie and Zollhoefer, Michael and Beeler, Thabo and Bernard, Florian and Bolkart, Timo and Kortylewski, Adam and Romdhani, Sami and Theobalt, Christian and Blanz, Volker and Vetter, Thomas},
	month = sep,
	year = {2020},
	pages = {1--38},
	file = {Egger 等。 - 2020 - 3D Morphable Face Models—Past, Present, and Future.pdf:C\:\\Users\\27637\\Zotero\\storage\\5U7XMKAT\\Egger 等。 - 2020 - 3D Morphable Face Models—Past, Present, and Future.pdf:application/pdf},
}

@article{utamiStudyFacialExpression2019,
	title = {A {Study} on {Facial} {Expression} {Recognition} in {Assessing} {Teaching} {Skills}: {Datasets} and {Methods}},
	volume = {161},
	issn = {18770509},
	shorttitle = {A {Study} on {Facial} {Expression} {Recognition} in {Assessing} {Teaching} {Skills}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050919318654},
	doi = {10.1016/j.procs.2019.11.154},
	abstract = {Abstract Facial expressions recognition (FER) is an important modality in the future assessing teaching skill system (ATS). The fundamental dFiafcfiearleenxcperwesisthioFnsErReciosginiittios nlo(wFErRes)oilsuatinoinmipmoargtaen, tumnioqduaeliotyccinlutshioenfuotuf rveeailsesdestseiancghteerasc, hainndgvsakriilel dsylsitgehmtin(Ag TcoSn).dTithioenfsu.nHdoamwevnetarl, idtiifsfesrteilnlcaeblweitoh sFoElvRe itsheinpritosbloewm roefsvoaluritaiotinonims ianghe,eaudnipqouseeso.cTchluisisotnudoyf,voevieleradllt,ebacehgearns,wainthdtvhaercieodllelicgthiotinnogfcaorntidcilteisonfrso.mHoSwcoepvuesr, citriesatsetidlltahbeleFEtoRsotalvxeonthoempyrocblalesms aonfdvmaraiaptpioendstihnehuesaedopfodseasta. sTehtsis. Astuftdeyr,foinvdeirnagll,obuetgwanhiwchithdathtaesceotsllwecetrieonsuoiftabrtlieclfeosrftrhoemimScaogpeuosf, ecxreparteesdsitohneiFnEteRacthaxinogn,oamdyeecplearssdiasncdusmsiaopnpaebdotuhtecluassesiofifedr-actlaassestisf.ieAr f(tuesrinfigntdhiengsaomuet wCKhi+chdadtaatsaeste)tws wasecraepsaubilteabolfesfoolvr itnhge pimroabglemosf (elxopwrersessionluitniotneaicmhiangge,, aodcecelupseirodni,scvuasrisaiotinonabionultigchlatsinsigficeor-ncdlaitsisoinfisera(nudsihnegatdheposasmese).CFKin+adllayt,afsreot)mwtahsecdapisacbulsesoiofnsoclovnindgucptreodb,leitmis k(lnoowwrnesthoeluptiootnenitmiaalgfeo,r omcocdluifsicoant,iovnaroifataiolgnorinithlmigsh,tianpgprcoopnrdiaitieodnastasnedtshaenad rpeosseeasr)c.hFoipnpalolryt,ufnriotimestfhoer dfuistucuressFioEnRcionnAduTcSt.ed, it is known the potential for modification of algorithms, appropriate datasets and research opportunities for future FER in ATS.},
	language = {en},
	urldate = {2020-11-07},
	journal = {Procedia Computer Science},
	author = {Utami, Pipit and Hartanto, Rudy and Soesanti, Indah},
	year = {2019},
	pages = {544--552},
	file = {Utami 等。 - 2019 - A Study on Facial Expression Recognition in Assess.pdf:C\:\\Users\\27637\\Zotero\\storage\\XCK6HENF\\Utami 等。 - 2019 - A Study on Facial Expression Recognition in Assess.pdf:application/pdf},
}

@article{agneseSurveyTaxonomyAdversarial2019,
	title = {A {Survey} and {Taxonomy} of {Adversarial} {Neural} {Networks} for {Text}-to-{Image} {Synthesis}},
	url = {http://arxiv.org/abs/1910.09399},
	abstract = {Text-to-image synthesis refers to computational methods which translate human written textual descriptions, in the form of keywords or sentences, into images with similar semantic meaning to the text. In earlier research, image synthesis relied mainly on word to image correlation analysis combined with supervised methods to ﬁnd best alignment of the visual content matching to the text. Recent progress in deep learning (DL) has brought a new set of unsupervised deep learning methods, particularly deep generative models which are able to generate realistic visual images using suitably trained neural network models. The change of direction from the computer vision based approaches to artiﬁcial intelligence (AI) driven methods ignited the intense interest in industry, such as virtual reality, recreational \& professional (eSports) gaming, and computer-aided design etc., to automatically generate compelling images from text-based natural language descriptions.},
	language = {en},
	urldate = {2020-11-07},
	journal = {arXiv:1910.09399 [cs]},
	author = {Agnese, Jorge and Herrera, Jonathan and Tao, Haicheng and Zhu, Xingquan},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.09399},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
	file = {Agnese 等。 - 2019 - A Survey and Taxonomy of Adversarial Neural Networ.pdf:C\:\\Users\\27637\\Zotero\\storage\\X5GRAER8\\Agnese 等。 - 2019 - A Survey and Taxonomy of Adversarial Neural Networ.pdf:application/pdf},
}

@article{wuSurveyImageSynthesis2017,
	title = {A survey of image synthesis and editing with generative adversarial networks},
	volume = {22},
	issn = {1007-0214},
	url = {http://ieeexplore.ieee.org/document/8195348/},
	doi = {10.23919/TST.2017.8195348},
	abstract = {This paper presents a survey of image synthesis and editing with Generative Adversarial Networks (GANs). GANs consist of two deep networks, a generator and a discriminator, which are trained in a competitive way. Due to the power of deep networks and the competitive training manner, GANs are capable of producing reasonable and realistic images, and have shown great capability in many image synthesis and editing applications. This paper surveys recent GAN papers regarding topics including, but not limited to, texture synthesis, image inpainting, image-to-image translation, and image editing.},
	language = {en},
	number = {6},
	urldate = {2020-11-07},
	journal = {Tsinghua Science and Technology},
	author = {Wu, Xian and Xu, Kun and Hall, Peter},
	month = dec,
	year = {2017},
	pages = {660--674},
	file = {Wu 等。 - 2017 - A survey of image synthesis and editing with gener.pdf:C\:\\Users\\27637\\Zotero\\storage\\KCUJMMQL\\Wu 等。 - 2017 - A survey of image synthesis and editing with gener.pdf:application/pdf},
}

@article{tsirikoglouSurveyImageSynthesis2020,
	title = {A {Survey} of {Image} {Synthesis} {Methods} for {Visual} {Machine} {Learning}},
	volume = {39},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.14047},
	doi = {10.1111/cgf.14047},
	abstract = {Image synthesis designed for machine learning applications provides the means to efficiently generate large quantities of training data while controlling the generation process to provide the best distribution and content variety. With the demands of deep learning applications, synthetic data have the potential of becoming a vital component in the training pipeline. Over the last decade, a wide variety of training data generation methods has been demonstrated. The potential of future development calls to bring these together for comparison and categorization. This survey provides a comprehensive list of the existing image synthesis methods for visual machine learning. These are categorized in the context of image generation, using a taxonomy based on modelling and rendering, while a classification is also made concerning the computer vision applications they are used. We focus on the computer graphics aspects of the methods, to promote future image generation for machine learning. Finally, each method is assessed in terms of quality and reported performance, providing a hint on its expected learning potential. The report serves as a comprehensive reference, targeting both groups of the applications and data development sides. A list of all methods and papers reviewed herein can be found at https:// computergraphics.on.liu.se/ image\_synthesis\_methods\_for\_visual\_machine\_learning/ .},
	language = {en},
	number = {6},
	urldate = {2020-11-07},
	journal = {Computer Graphics Forum},
	author = {Tsirikoglou, A. and Eilertsen, G. and Unger, J.},
	month = sep,
	year = {2020},
	pages = {426--451},
	file = {Tsirikoglou 等。 - 2020 - A Survey of Image Synthesis Methods for Visual Mac.pdf:C\:\\Users\\27637\\Zotero\\storage\\TRMC4923\\Tsirikoglou 等。 - 2020 - A Survey of Image Synthesis Methods for Visual Mac.pdf:application/pdf},
}

@article{khanSurveyAnalysisHuman2020,
	title = {A survey on analysis of human faces and facial expressions datasets},
	volume = {11},
	issn = {1868-8071, 1868-808X},
	url = {http://link.springer.com/10.1007/s13042-019-00995-6},
	doi = {10.1007/s13042-019-00995-6},
	abstract = {Facial expressions are the basic input for visual emotion detection. They are of great importance in computer vision society. In last decade, substantial amount of work has been done in the field of facial expressions datasets. This survey covers all of the publically available databases in detail and provides necessary information about these sets. This review delivers comprehensive support to researchers in selection of their desired dataset. The datasets are organized in decreasing order of their importance with respect to diversity in expressions, poses, number of images and resolution. This survey also provides comprehensive tabular comparison of different face based databases.},
	language = {en},
	number = {3},
	urldate = {2020-11-07},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Khan, Gulraiz and Samyan, Sahar and Khan, Muhammad Usman Ghani and Shahid, Muhammad and Wahla, Samyan Qayyum},
	month = mar,
	year = {2020},
	pages = {553--571},
	file = {Khan 等。 - 2020 - A survey on analysis of human faces and facial exp.pdf:C\:\\Users\\27637\\Zotero\\storage\\FAWJQU4H\\Khan 等。 - 2020 - A survey on analysis of human faces and facial exp.pdf:application/pdf},
}

@article{shortenSurveyImageData2019,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	language = {en},
	number = {1},
	urldate = {2020-11-07},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	month = dec,
	year = {2019},
	pages = {60},
	file = {Shorten 和 Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:C\:\\Users\\27637\\Zotero\\storage\\RRGE74KG\\Shorten 和 Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:application/pdf},
}

@article{CompoundFacialExpressions2015,
	title = {Compound facial expressions of emotion: from basic research to clinical applications},
	volume = {17},
	issn = {26083477, 12948322},
	shorttitle = {Compound facial expressions of emotion},
	url = {https://www.dialogues-cns.org/contents-17-4/dialoguesclinneurosci-17-443/},
	doi = {10.31887/DCNS.2015.17.4/sdu},
	abstract = {Emotions are sometimes revealed through facial expressions. When these natural facial articulations involve the contraction of the same muscle groups in people of distinct cultural upbringings, this is taken as evidence of a biological origin of these emotions. While past research had identified facial expressions associated with a single internally felt category (eg, the facial expression of happiness when we feel joyful), we have recently studied facial expressions observed when people experience compound emotions (eg, the facial expression of happy surprise when we feel joyful in a surprised way, as, for example, at a surprise birthday party). Our research has identified 17 compound expressions consistently produced across cultures, suggesting that the number of facial expressions of emotion of biological origin is much larger than previously believed. The present paper provides an overview of these findings and shows evidence supporting the view that spontaneous expressions are produced using the same facial articulations previously identified in laboratory experiments. We also discuss the implications of our results in the study of psychopathologies, and consider several open research questions.},
	language = {en},
	number = {4},
	urldate = {2020-11-07},
	journal = {Dialogues in Clinical Neuroscience},
	month = dec,
	year = {2015},
	pages = {443--455},
	file = {2015 - Compound facial expressions of emotion from basic.pdf:C\:\\Users\\27637\\Zotero\\storage\\AKECCGYQ\\2015 - Compound facial expressions of emotion from basic.pdf:application/pdf},
}

@inproceedings{benitez-quirozEmotioNetAccurateRealTime2016,
	address = {Las Vegas, NV, USA},
	title = {{EmotioNet}: {An} {Accurate}, {Real}-{Time} {Algorithm} for the {Automatic} {Annotation} of a {Million} {Facial} {Expressions} in the {Wild}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {{EmotioNet}},
	url = {http://ieeexplore.ieee.org/document/7780969/},
	doi = {10.1109/CVPR.2016.600},
	language = {en},
	urldate = {2020-11-07},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Benitez-Quiroz, C. Fabian and Srinivasan, Ramprakash and Martinez, Aleix M.},
	month = jun,
	year = {2016},
	pages = {5562--5570},
	file = {Benitez-Quiroz 等。 - 2016 - EmotioNet An Accurate, Real-Time Algorithm for th.pdf:C\:\\Users\\27637\\Zotero\\storage\\WEDBSSZH\\Benitez-Quiroz 等。 - 2016 - EmotioNet An Accurate, Real-Time Algorithm for th.pdf:application/pdf},
}

@article{huangFacialExpressionRecognition2019,
	title = {Facial expression recognition: a survey},
	volume = {11},
	issn = {2073-8994},
	shorttitle = {Facial expression recognition},
	url = {https://www.mdpi.com/2073-8994/11/10/1189},
	doi = {10.3390/sym11101189},
	abstract = {Facial Expression Recognition (FER), as the primary processing method for non-verbal intentions, is an important and promising ﬁeld of computer vision and artiﬁcial intelligence, and one of the subject areas of symmetry. This survey is a comprehensive and structured overview of recent advances in FER. We ﬁrst categorise the existing FER methods into two main groups, i.e., conventional approaches and deep learning-based approaches. Methodologically, to highlight the differences and similarities, we propose a general framework of a conventional FER approach and review the possible technologies that can be employed in each component. As for deep learning-based methods, four kinds of neural network-based state-of-the-art FER approaches are presented and analysed. Besides, we introduce seventeen commonly used FER datasets and summarise four FER-related elements of datasets that may inﬂuence the choosing and processing of FER approaches. Evaluation methods and metrics are given in the later part to show how to assess FER algorithms, along with subsequent performance comparisons of different FER approaches on the benchmark datasets. At the end of the survey, we present some challenges and opportunities that need to be addressed in future.},
	language = {en},
	number = {10},
	urldate = {2020-11-07},
	journal = {Symmetry},
	author = {Huang, Yunxin and Chen, Fei and Lv, Shaohe and Wang, Xiaodong},
	month = sep,
	year = {2019},
	pages = {1189},
	file = {Huang 等。 - 2019 - Facial Expression Recognition A Survey.pdf:C\:\\Users\\27637\\Zotero\\storage\\UUUEBKRU\\Huang 等。 - 2019 - Facial Expression Recognition A Survey.pdf:application/pdf},
}

@article{qiaoReferringExpressionComprehension,
	title = {Referring {Expression} {Comprehension}: {A} {Survey} of {Methods} and {Datasets}},
	abstract = {Referring expression comprehension (REC) aims to localize a target object in an image described by a referring expression phrased in natural language. Different from the object detection task that queried object labels have been pre-deﬁned, the REC problem only can observe the queries during the test. It thus more challenging than a conventional computer vision problem. This task has attracted a lot of attention from both computer vision and natural language processing community, and several lines of work have been proposed, from CNN-RNN model, modular network to complex graph-based model. In this survey, we ﬁrst examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to encode the visual and textual modalities. In particular, we examine the common approach of joint embedding images and expressions to a common feature space. We also discuss modular architectures and graph-based models that interface with structured graph representation. In the second part of this survey, we review the datasets available for training and evaluating REC systems. We then group results according to the datasets, backbone models, settings so that they can be fairly compared. Finally, we discuss promising future directions for the ﬁeld, in particular the compositional referring expression comprehension that requires longer reasoning chain to address.},
	language = {en},
	author = {Qiao, Yanyuan and Deng, Chaorui and Wu, Qi},
	pages = {13},
	file = {Qiao 等。 - Referring Expression Comprehension A Survey of Me.pdf:C\:\\Users\\27637\\Zotero\\storage\\TLV3AERM\\Qiao 等。 - Referring Expression Comprehension A Survey of Me.pdf:application/pdf},
}

@article{htaySurveyEmotionRecognition2019,
	title = {Survey on {Emotion} {Recognition} {Using} {Facial} {Expression}},
	volume = {33},
	abstract = {Automatic recognition of human affects has become more interesting and challenging problem in artificial intelligence, human-computer interaction and computer vision fields. Facial Expression (FE) is the one of the most significant features to recognize the emotion of human in daily human interaction. FE Recognition (FER) has received important interest from psychologists and computer scientists for the applications of health care assessment, human affect analysis, and human computer interaction. Human express their emotions in a number of ways including body gesture, word, vocal and facial expressions. Expression is the important channel to convey emotion information of different people because face can express mainly human emotion. This paper surveys the current research works related to facial expression recognition. The study attends to explored details of the facial datasets, feature extraction methods, the comparison results and futures studies of the facial emotion system.},
	language = {en},
	number = {1},
	author = {Htay, Moe Moe and Win, Zin Mar},
	year = {2019},
	pages = {10},
	file = {Htay 和 Win - 2019 - Survey on Emotion Recognition Using Facial Express.pdf:C\:\\Users\\27637\\Zotero\\storage\\4M9Q7W77\\Htay 和 Win - 2019 - Survey on Emotion Recognition Using Facial Express.pdf:application/pdf},
}

@article{cabanacWhatEmotion2002,
	title = {What is emotion?},
	volume = {60},
	issn = {03766357},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0376635702000785},
	doi = {10.1016/S0376-6357(02)00078-5},
	language = {en},
	number = {2},
	urldate = {2020-11-07},
	journal = {Behavioural Processes},
	author = {Cabanac, Michel},
	month = nov,
	year = {2002},
	pages = {69--83},
}

@article{adolphsInvestigatingEmotionsFunctional2018,
	title = {Investigating {Emotions} as {Functional} {States} {Distinct} {From} {Feelings}},
	volume = {10},
	issn = {1754-0739, 1754-0747},
	url = {http://journals.sagepub.com/doi/10.1177/1754073918765662},
	doi = {10.1177/1754073918765662},
	language = {en},
	number = {3},
	urldate = {2020-11-08},
	journal = {Emotion Review},
	author = {Adolphs, Ralph and Andler, Daniel},
	month = jul,
	year = {2018},
	pages = {191--201},
	file = {已接受版本:C\:\\Users\\27637\\Zotero\\storage\\8MTW6JPL\\Adolphs 和 Andler - 2018 - Investigating Emotions as Functional States Distin.pdf:application/pdf},
}

@article{ekmanArgumentBasicEmotions1992,
	title = {An argument for basic emotions},
	volume = {6},
	issn = {0269-9931, 1464-0600},
	url = {https://www.tandfonline.com/doi/full/10.1080/02699939208411068},
	doi = {10.1080/02699939208411068},
	language = {en},
	number = {3-4},
	urldate = {2020-11-08},
	journal = {Cognition and Emotion},
	author = {Ekman, Paul},
	month = may,
	year = {1992},
	pages = {169--200},
}

@article{cowieEmotionRecognitionHumancomputer2001,
	title = {Emotion recognition in human-computer interaction},
	volume = {18},
	issn = {10535888},
	url = {http://ieeexplore.ieee.org/document/911197/},
	doi = {10.1109/79.911197},
	number = {1},
	urldate = {2020-11-08},
	journal = {IEEE Signal Processing Magazine},
	author = {Cowie, R. and Douglas-Cowie, E. and Tsapatsoulis, N. and Votsis, G. and Kollias, S. and Fellenz, W. and Taylor, J.G.},
	month = jan,
	year = {2001},
	pages = {32--80},
}

@article{chenAugmentedRealitybasedSelffacial2015,
	title = {Augmented reality-based self-facial modeling to promote the emotional expression and social skills of adolescents with autism spectrum disorders},
	volume = {36},
	issn = {08914222},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0891422214004296},
	doi = {10.1016/j.ridd.2014.10.015},
	language = {en},
	urldate = {2020-11-08},
	journal = {Research in Developmental Disabilities},
	author = {Chen, Chien-Hsu and Lee, I-Jui and Lin, Ling-Yi},
	month = jan,
	year = {2015},
	pages = {396--403},
}

@article{bekeleUnderstandingHowAdolescents2013,
	title = {Understanding {How} {Adolescents} with {Autism} {Respond} to {Facial} {Expressions} in {Virtual} {Reality} {Environments}},
	volume = {19},
	issn = {1077-2626},
	url = {http://ieeexplore.ieee.org/document/6479212/},
	doi = {10.1109/TVCG.2013.42},
	number = {4},
	urldate = {2020-11-08},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Bekele, Esubalew and Zheng, Zhi and Swanson, Amy and Crittendon, Julie and Warren, Zachary and Sarkar, Nilanjan},
	month = apr,
	year = {2013},
	pages = {711--720},
	file = {已接受版本:C\:\\Users\\27637\\Zotero\\storage\\RL5RJZ22\\Bekele 等。 - 2013 - Understanding How Adolescents with Autism Respond .pdf:application/pdf},
}

@article{jabonFacialExpressionAnalysis2011,
	title = {Facial expression analysis for predicting unsafe driving behavior},
	volume = {10},
	issn = {1536-1268},
	url = {http://ieeexplore.ieee.org/document/5456355/},
	doi = {10.1109/MPRV.2010.46},
	number = {4},
	urldate = {2020-11-08},
	journal = {IEEE Pervasive Computing},
	author = {Jabon, M. E. and Bailenson, J. N. and Pontikakis, E. and Takayama, L. and Nass, C.},
	month = apr,
	year = {2011},
	pages = {84--95},
}

@inproceedings{lankesFacialExpressionsGame2008,
	address = {Yokohama, Japan},
	title = {Facial expressions as game input with different emotional feedback conditions},
	isbn = {978-1-60558-393-8},
	url = {http://portal.acm.org/citation.cfm?doid=1501750.1501809},
	doi = {10.1145/1501750.1501809},
	language = {en},
	urldate = {2020-11-08},
	booktitle = {Proceedings of the 2008 {International} {Conference} in {Advances} on {Computer} {Entertainment} {Technology} - {ACE} '08},
	publisher = {ACM Press},
	author = {Lankes, Michael and Riegler, Stefan and Weiss, Astrid and Mirlacher, Thomas and Pirker, Michael and Tscheligi, Manfred},
	year = {2008},
	pages = {253},
}

@inproceedings{jerrittaPhysiologicalSignalsBased2011,
	address = {Penang, Malaysia},
	title = {Physiological signals based human emotion {Recognition}: a review},
	isbn = {978-1-61284-414-5},
	shorttitle = {Physiological signals based human emotion {Recognition}},
	url = {http://ieeexplore.ieee.org/document/5759912/},
	doi = {10.1109/CSPA.2011.5759912},
	urldate = {2020-11-08},
	booktitle = {2011 {IEEE} 7th {International} {Colloquium} on {Signal} {Processing} and its {Applications}},
	publisher = {IEEE},
	author = {Jerritta, S and Murugappan, M and Nagarajan, R and Wan, Khairunizam},
	month = mar,
	year = {2011},
	pages = {410--415},
}
